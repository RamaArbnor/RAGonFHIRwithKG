{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neo4j import GraphDatabase\n",
    "\n",
    "# # Initialize the Neo4j driver\n",
    "# driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "# def print_embeddable_nodes(driver):\n",
    "#     query = \"\"\"\n",
    "#         MATCH (n:Embeddable)\n",
    "#         RETURN n.resourceType AS resourceType, n.embeddingText AS embeddingText, n.embedding as emb\n",
    "#     \"\"\"\n",
    "    \n",
    "#     with driver.session() as session:\n",
    "#         result = session.run(query)\n",
    "#         for record in result:\n",
    "#             resource_type = record.get(\"resourceType\", \"Unknown\")\n",
    "#             embedding_text = record.get(\"embeddingText\", \"No embedding text\")\n",
    "#             embedding = record.get(\"emb\", \"No embedding text\")\n",
    "#             print(f\"Resource Type: {resource_type}, Embedding Text: {embedding_text[:100]}..., \\nEmbedding: {embedding[:2]}...\")  # Print first 100 chars\n",
    "\n",
    "# # Call the function to print embeddable nodes\n",
    "# print_embeddable_nodes(driver)\n",
    "\n",
    "# # Close the driver connection\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# query_embedding = hf_embeddings.embed_query(query)\n",
    "# print(\"Query Embedding:\", query_embedding)\n",
    "\n",
    "# query = f\"\"\" \n",
    "#     MATCH (n:Condition)\n",
    "#     WHERE n.embedding IS NOT NULL\n",
    "#     WITH n, vector.similarity.cosine(n.embedding, {query_embedding}) AS score\n",
    "#     RETURN n, score\n",
    "#     ORDER BY score DESC\n",
    "#     LIMIT 10\n",
    "# \"\"\"\n",
    "\n",
    "# # Execute the query\n",
    "# with driver.session() as session:\n",
    "#     result = session.run(query)\n",
    "#     for record in result:\n",
    "#         print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama emebedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.embeddings.base import Embeddings\n",
    "# import requests  # Or use any necessary package to communicate with Ollama\n",
    "\n",
    "# class OllamaEmbeddings(Embeddings):\n",
    "#     def __init__(self, model_name=\"mxbai-embed-large\"):\n",
    "#         self.model_name = model_name\n",
    "\n",
    "#     def embed_text(self, text: str) -> list:\n",
    "#         # Call Ollama's API or local embedding function\n",
    "#         # Modify this code based on how you interact with Ollama\n",
    "#         response = requests.post(\n",
    "#             \"http://localhost:11434/api/embeddings\",  # Replace with Ollama's endpoint\n",
    "#             json={\"model\": self.model_name, \"text\": text}\n",
    "#         )\n",
    "#         if response.status_code == 200:\n",
    "#             return response.json()[\"embedding\"]\n",
    "#         else:\n",
    "#             raise ValueError(\"Error retrieving embedding from Ollama\")\n",
    "\n",
    "#     def embed_documents(self, texts: list) -> list:\n",
    "#         return [self.embed_text(text) for text in texts]\n",
    "    \n",
    "#     def embed_query(self, text: str) -> list:\n",
    "#         # Use the same embedding process for queries as for text\n",
    "#         return self.embed_text(text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
